{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Spark\n",
    "====\n",
    "\n",
    "This lecture is an introduction to the Spark framework for distributed computing, the basic data and control flow abstractions, and getting comfortable with the functional programming style needed to write a Spark application.\n",
    "\n",
    "- What problem does Spark solve?\n",
    "- SparkContext and the master configuration\n",
    "- RDDs\n",
    "- Actions\n",
    "- Transforms\n",
    "- Key-value RDDs\n",
    "- Example - word count\n",
    "- Persistence\n",
    "- Merging key-value RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning objectives\n",
    "----\n",
    "\n",
    "- Overview of Spark\n",
    "- Working with Spark RDDs\n",
    "- Actions and transforms\n",
    "- Working with Spark DataFrames\n",
    "- Using the `ml` and `mllib` for machine learning\n",
    "\n",
    "#### Not covered\n",
    "\n",
    "- Spark GraphX (library for graph algorithms)\n",
    "- Spark Streaming (library for streaming (microbatch) data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation\n",
    "----\n",
    "\n",
    "You should use the current version of Spark at https://spark.apache.org/downloads.html. Choose the package `Pre-built for Hadoop2.7 and later`. The instructions below use the version current as of 9 April 2018.\n",
    "```bash\n",
    "cd ~\n",
    "wget https://www.apache.org/dyn/closer.lua/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\n",
    "tar spark-2.3.0-bin-hadoop2.7.tgz\n",
    "rm spark-2.3.0-bin-hadoop2.7.tgz\n",
    "mv spark-2.3.0-bin-hadoop2.7 spark\n",
    "```\n",
    "\n",
    "Install the `py4j` Python package needed for `pyspark`\n",
    "```\n",
    "pip install py4j\n",
    "```\n",
    "\n",
    "You need to define these environment variables before starting the notebook.\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=~/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "export PYSPARK_SUBMIT_ARGS=\"--packages ${PACKAGES} pyspark-shell\"\n",
    "```\n",
    "\n",
    "In Unix/Mac, this can be done in `.bashrc` or `.bash_profile`.\n",
    "\n",
    "For the adventurous, see [Running Spark on an AWS EMR cluster](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "----\n",
    "\n",
    "- [Quick Start](http://spark.apache.org/docs/latest/quick-start.html)\n",
    "- [Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n",
    "- [DataFramews, DataSets and SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [MLLib](http://spark.apache.org/docs/latest/mllib-guide.html)\n",
    "- [GraphX](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n",
    "- [Streaming](http://spark.apache.org/docs/latest/streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Spark\n",
    "----\n",
    "\n",
    "With massive data, we need to load, extract, transform and analyze the data on multiple computers to overcome I/O and processing bottlenecks. However, when working on multiple computers (possibly hundreds to thousands), there is a high risk of failure in one or more nodes. Distributed computing frameworks are designed to handle failures gracefully, allowing the developer to focus on algorithm development rather than system administration.\n",
    "\n",
    "The first such widely used open source framework was the Hadoop MapReduce framework. This provided transparent fault tolerance, and popularized the functional programming approach to distributed computing. The Hadoop work-flow uses repeated invocations of the following instructions:\n",
    "\n",
    "```\n",
    "load dataset from disk to memory\n",
    "map function to elements of dataset\n",
    "reduce results of map to get new aggregate dataset\n",
    "save new dataset to disk\n",
    "```\n",
    "\n",
    "Hadoop has two main limitations:\n",
    "\n",
    "- the repeated saving and loading of data to disk can be slow, and makes interactive development very challenging\n",
    "- restriction to only `map` and `reduce` constructs results in increased code complexity, since every problem must be tailored to the `map-reduce` format\n",
    "\n",
    "Spark is a more recent framework for distributed computing that addresses the limitations of Hadoop by allowing the use of in-memory datasets for iterative computation, and providing a rich set of functional programming constructs to make the developer's job easier. Spark also provides libraries for common big data tasks, such as the need to run SQL queries, perform machine learning and process large graphical structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Languages supported\n",
    "----\n",
    "\n",
    "Fully supported\n",
    "\n",
    "- Java\n",
    "- Scala\n",
    "- Python\n",
    "- R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed computing bakkground\n",
    "\n",
    "With distributed computing, you interact with a network of computers that communicate via message passing as if issuing instructions to a single computer.\n",
    "\n",
    "![Distributed computing](https://image.slidesharecdn.com/distributedcomputingwithspark-150414042905-conversion-gate01/95/distributed-computing-with-spark-21-638.jpg?)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/distributedcomputingwithspark-150414042905-conversion-gate01/95/distributed-computing-with-spark-21-638.jpg\n",
    "\n",
    "### Hadoop and Spark\n",
    "\n",
    "- There are 3 major components to a distributed system\n",
    "    - storage\n",
    "    - cluster management\n",
    "    - computing engine\n",
    "\n",
    "- Hadoop is a framework that provides all 3 \n",
    "    - distributed storage (HDFS) \n",
    "    - clsuter managemnet (YARN)\n",
    "    - computing eneine (MapReduce)\n",
    "    \n",
    "- Spakr only provides the (in-memory) distributed computing engine, and relies on other frameworks for storage and clsuter manageemnt. It is most frequently used on top of the Hadoop framework, but can also use other distribtued storage(e.g. S3 and Cassandra) or cluster mangement (e.g. Mesos) software.\n",
    "\n",
    "### Distributed stoage\n",
    "\n",
    "![storage](http://slideplayer.com/slide/3406872/12/images/15/HDFS+Framework+Key+features+of+HDFS:.jpg)\n",
    "\n",
    "Source: http://slideplayer.com/slide/3406872/12/images/15/HDFS+Framework+Key+features+of+HDFS:.jpg\n",
    "\n",
    "### Role of YARN\n",
    "\n",
    "- Resource manageer (manages cluster resources)\n",
    "    - Scheduler\n",
    "    - Applicaitons manager\n",
    "- Ndoe manager (manages single machine/node)\n",
    "    - manages data containers/partitions\n",
    "    - monitors reosurce usage\n",
    "    - reprots to resource manager\n",
    "\n",
    "![Yarn](https://kannandreams.files.wordpress.com/2013/11/yarn1.png)\n",
    "\n",
    "Source: https://kannandreams.files.wordpress.com/2013/11/yarn1.png\n",
    "\n",
    "### YARN operations\n",
    "\n",
    "![yarn ops](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)\n",
    "\n",
    "Source: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif\n",
    "\n",
    "### Hadoop MapReduce versus Spark\n",
    "\n",
    "Spark has several advantages over Hadoop MapReduce\n",
    "\n",
    "- Use of RAM rahter than disk mean fsater processing for multi-step operations\n",
    "- Allows interactive applicaitons\n",
    "- Allows real-time applications\n",
    "- More flexible programming API (full range of functional constructs)\n",
    "\n",
    "![Hadoop](https://i0.wp.com/s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png)\n",
    "\n",
    "Source: https://i0.wp.com/s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png\n",
    "\n",
    "### Overall Ecosystem\n",
    "\n",
    "![spark](https://cdn-images-1.medium.com/max/1165/1*z0Vm749Pu6mHdlyPsznMRg.png)\n",
    "\n",
    "Source: https://cdn-images-1.medium.com/max/1165/1*z0Vm749Pu6mHdlyPsznMRg.png\n",
    "\n",
    "### Spark Ecosystem\n",
    "\n",
    "- Spark is written in Scala, a functional programming language built on top of the Java Virtual Machine (JVM)\n",
    "- Traditionally, you have to code in Scala to get the best performacne from Spark\n",
    "- With Spark DataFrames and vectorized operations (Spark 2.3 onwards) Python is now competitive\n",
    "\n",
    "![eco](https://data-flair.training/blogs/wp-content/uploads/apache-spark-ecosystem-components.jpg)\n",
    "\n",
    "Source: https://data-flair.training/blogs/wp-content/uploads/apache-spark-ecosystem-components.jpg\n",
    "\n",
    "### Livy and Spark magic\n",
    "\n",
    "- Livy provides a REST interface to a Spark cluster.\n",
    "\n",
    "![Livy](https://cdn-images-1.medium.com/max/956/0*-lwKpnEq0Tpi3Tlj.png)\n",
    "\n",
    "Source: https://cdn-images-1.medium.com/max/956/0*-lwKpnEq0Tpi3Tlj.png\n",
    "\n",
    "### PySpark\n",
    "\n",
    "![PySpark](http://i.imgur.com/YlI8AqEl.png)\n",
    "\n",
    "Source: http://i.imgur.com/YlI8AqEl.png\n",
    "\n",
    "### Resilident distributed datasets (RDDs)\n",
    "\n",
    "![rdd](https://mapr.com/blog/real-time-streaming-data-pipelines-apache-apis-kafka-spark-streaming-and-hbase/assets/blogimages/msspark/imag12.png)\n",
    "\n",
    "Source: https://mapr.com/blog/real-time-streaming-data-pipelines-apache-apis-kafka-spark-streaming-and-hbase/assets/blogimages/msspark/imag12.png\n",
    "\n",
    "### Spark fault tolerance\n",
    "\n",
    "![graph](https://image.slidesharecdn.com/deep-dive-with-spark-streamingtathagata-dasspark-meetup2013-06-17-130623151510-phpapp02/95/deep-dive-with-spark-streaming-tathagata-das-spark-meetup-20130617-13-638.jpg)\n",
    "\n",
    "Source: https://image.slidesharecdn.com/deep-dive-with-spark-streamingtathagata-dasspark-meetup2013-06-17-130623151510-phpapp02/95/deep-dive-with-spark-streaming-tathagata-das-spark-meetup-20130617-13-638.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>136</td><td>application_1522938745830_0062</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0062/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0062_01_000001/user06021\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '2048M', 'executorCores': 2, 'proxyUser': 'user06021', 'conf': {'spark.master': 'yarn-client'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>136</td><td>application_1522938745830_0062</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0062/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0062_01_000001/user06021\">Link</a></td><td>✔</td></tr><tr><td>137</td><td>application_1522938745830_0068</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0068/\">Link</a></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring allocated resources\n",
    "\n",
    "Note the proxyUser from `%%info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>119</td><td>application_1522938745830_0044</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0044/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-2174.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0044_01_000001/user06021\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '2G', 'numExecutors': 10, 'executorCores': 2, 'executorMemory': '2048M', 'proxyUser': 'user06021', 'conf': {'spark.master': 'yarn'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>115</td><td>application_1522938745830_0039</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0039/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-3547.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0039_01_000001/user06021\">Link</a></td><td></td></tr><tr><td>116</td><td>application_1522938745830_0040</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0040/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-2171.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0040_01_000001/user06021\">Link</a></td><td></td></tr><tr><td>119</td><td>application_1522938745830_0044</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0044/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-2174.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0044_01_000001/user06021\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "     {\"driverMemory\": \"2G\", \n",
    "      \"numExecutors\": 10, \n",
    "      \"executorCores\": 2, \n",
    "      \"executorMemory\": \"2048M\", \n",
    "      \"proxyUser\": \"user06021\",\n",
    "      \"conf\": {\"spark.master\": \"yarn\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version\n",
    "\n",
    "The default version of Python with the PySpark kernel is Python 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=2, minor=7, micro=12, releaselevel='final', serial=0)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to shut down the notebook after use\n",
    "\n",
    "When you are done running Sark jobs with this notebook, go to the  notebook's file menu, and select the \"Close and Halt\" option to terminate the notebook's kernel and clear the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
